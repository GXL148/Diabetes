{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 183,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import ExtraTreesRegressor\n",
    "from sklearn.linear_model import ElasticNetCV, RidgeCV\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.neighbors import KNeighborsRegressor\n",
    "from sklearn.pipeline import make_pipeline, make_union, Pipeline\n",
    "from sklearn.feature_selection import VarianceThreshold\n",
    "from sklearn.metrics import mean_squared_error\n",
    "from sklearn.preprocessing import MinMaxScaler\n",
    "from sklearn.svm import SVR,SVC\n",
    "from sklearn.linear_model import LinearRegression, ElasticNet,Ridge,Lasso,RidgeClassifierCV\n",
    "from sklearn.model_selection import KFold  \n",
    "from sklearn.feature_selection import SelectPercentile, f_regression\n",
    "import lightgbm as lgb\n",
    "from sklearn.ensemble import GradientBoostingRegressor\n",
    "import copy as cp\n",
    "from sklearn.preprocessing import MaxAbsScaler, Normalizer\n",
    "from sklearn.svm import LinearSVR\n",
    "from tpot.builtins import StackingEstimator\n",
    "from imblearn.under_sampling import NearMiss ,RandomUnderSampler, NeighbourhoodCleaningRule, OneSidedSelection, AllKNN\n",
    "from imblearn.over_sampling import SMOTE\n",
    "from xgboost import XGBRegressor\n",
    "from imblearn.ensemble import EasyEnsemble \n",
    "from sklearn.metrics import f1_score, accuracy_score\n",
    "from sklearn.utils import shuffle\n",
    "from imblearn.over_sampling import SMOTE, ADASYN\n",
    "from sklearn.ensemble import BaggingClassifier\n",
    "from sklearn.tree import DecisionTreeClassifier\n",
    "from imblearn.ensemble import BalancedBaggingClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 184,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train = pd.read_csv(\"../data/processed/train.csv\")\n",
    "test = pd.read_csv(\"../data/processed/test.csv\")\n",
    "test.pop(\"id\")\n",
    "train.pop(\"id\")\n",
    "target = train.pop(\"血糖\")\n",
    "train_x = train.as_matrix()\n",
    "train_y = target.as_matrix()\n",
    "test_x = test.as_matrix()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 185,
   "metadata": {},
   "outputs": [],
   "source": [
    "high_labels = np.zeros((train_y.shape[0],))\n",
    "for i in range(train_y.shape[0]):\n",
    "    if train_y[i]<6.68:\n",
    "        high_labels[i] = 1\n",
    "    else:\n",
    "        high_labels[i] = -1\n",
    "low_labels = np.zeros((train_y.shape[0],))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin, ClassifierMixin\n",
    "from sklearn.utils import check_array\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "class myStackingFeaturesClassifer(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.estimator = None\n",
    "        self.lgb = lgb.LGBMClassifier(boosting_type=\"GBDT\",\n",
    "                                      num_leaves=31,\n",
    "                                      learning_rate=0.01,\n",
    "                                      feature_fraction=0.5,\n",
    "                                      bagging_fraction=0.5,\n",
    "                                      bagging_freq=5,\n",
    "                                      n_estimators=400)\n",
    "        self.grd_enc = OneHotEncoder()\n",
    "        self.lr = RidgeCV()\n",
    "        self.classes_ = [-1,1]\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        self.lgb.fit(X, y)\n",
    "        self.grd_enc.fit(self.lgb.apply(X))\n",
    "        self.lr.fit(self.grd_enc.transform(self.lgb.apply(X)), y)\n",
    "    def predict_proba(self, X):\n",
    "        return self.lr.predict_proba(self.grd_enc.transform(self.lgb.apply(X)))\n",
    "    def predict(self, X):\n",
    "        return self.lr.predict(self.grd_enc.transform(self.lgb.apply(X)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 186,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def modif_value(training_features, training_target, training_labels, testing_features, X, Y):\n",
    "    exported_pipeline = classifier = Pipeline([\n",
    "    (\"scaler\",MaxAbsScaler()),\n",
    "    (\"SVR\",StackingEstimator(estimator=SVC())),\n",
    "    (\"RidgeCV\",StackingEstimator(estimator=RidgeClassifierCV())),\n",
    "                                    (\"BaggingClassifier\",BaggingClassifier(base_estimator=myStackingFeaturesClassifer(),random_state=201801))])\n",
    "    exported_pipeline.fit(training_features, training_labels)\n",
    "    prob = exported_pipeline.predict_proba(testing_features)\n",
    "    \n",
    "    \n",
    "    predicts = np.zeros((prob.shape[0],))\n",
    "    for i in range(prob.shape[0]):\n",
    "        if prob[i,1]>0.5:\n",
    "            predicts[i] = 1\n",
    "        else:\n",
    "            predicts[i] = -1\n",
    "    negative_pred_list = list(np.where(predicts==-1)[0])\n",
    "    negative_labels_list = list(np.where(training_labels==-1)[0])\n",
    "    \n",
    "\n",
    "    \n",
    "    negative_results = None\n",
    "\n",
    "    \n",
    "    if len(negative_pred_list)==0:\n",
    "        negative_results = []\n",
    "    else:\n",
    "        exported_pipeline = Pipeline([\n",
    "            (\"scaler\", MaxAbsScaler()),\n",
    "            (\"SVR\", StackingEstimator(estimator=LinearSVR(C=0.01, dual=False, epsilon=1.0, loss=\"squared_epsilon_insensitive\", tol=0.001))),\n",
    "            (\"RidgeCV\", StackingEstimator(estimator=RidgeCV())),\n",
    "            (\"LGB\", lgb.LGBMRegressor(objective='regression',\n",
    "                                      boosting_type=\"GBDT\",\n",
    "                                      num_leaves=31,\n",
    "                                      learning_rate=0.01,\n",
    "                                      feature_fraction=0.5,\n",
    "                                      bagging_fraction=0.5,\n",
    "                                      bagging_freq=5,\n",
    "                                      n_estimators=400))]\n",
    "        )\n",
    "        print(len(negative_labels_list))\n",
    "        exported_pipeline.fit(X, Y)\n",
    "        negative_results = exported_pipeline.predict(testing_features[negative_pred_list])  \n",
    "        \n",
    "    positive_pred_list = list(np.where(predicts==1)[0])\n",
    "    positive_labels_list = list(np.where(training_labels==1)[0])\n",
    "    positive_results = None\n",
    "    '''   \n",
    "    if len(positive_pred_list)==0:\n",
    "        positive_results = []\n",
    "    else:\n",
    "        exported_pipeline = Pipeline([\n",
    "            (\"scaler\", MaxAbsScaler()),\n",
    "            (\"SVR\", StackingEstimator(\n",
    "                estimator=LinearSVR(C=0.01, dual=False, epsilon=1.0, loss=\"squared_epsilon_insensitive\", tol=0.001))),\n",
    "            (\"RidgeCV\", StackingEstimator(estimator=RidgeCV())),\n",
    "            (\"LGB\", lgb.LGBMRegressor(objective='regression',\n",
    "                                      boosting_type=\"GBDT\",\n",
    "                                      num_leaves=31,\n",
    "                                      learning_rate=0.01,\n",
    "                                      feature_fraction=0.5,\n",
    "                                      bagging_fraction=0.5,\n",
    "                                      bagging_freq=5,\n",
    "                                      n_estimators=400))]\n",
    "        )\n",
    "        \n",
    "        exported_pipeline.fit(training_features[positive_labels_list], training_target[positive_labels_list])\n",
    "        positive_results = exported_pipeline.predict(testing_features[positive_pred_list])  \n",
    "    '''\n",
    "    \n",
    "    \n",
    "    return negative_results,negative_pred_list, positive_results,positive_pred_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 194,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.80242999999999998"
      ]
     },
     "execution_count": 194,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.base import BaseEstimator, TransformerMixin, ClassifierMixin\n",
    "from sklearn.utils import check_array\n",
    "from sklearn.preprocessing import OneHotEncoder\n",
    "\n",
    "class myStackingFeatures(BaseEstimator, TransformerMixin):\n",
    "    def __init__(self):\n",
    "        self.estimator = None\n",
    "        self.lgb = GradientBoostingRegressor(loss='ls', alpha=0.9,\n",
    "                                    n_estimators=100,\n",
    "                                    learning_rate=0.02,\n",
    "                                    max_depth=8,\n",
    "                                    subsample=0.8,\n",
    "                                    min_samples_split=9,\n",
    "                                    max_leaf_nodes=10)\n",
    "        self.grd_enc = OneHotEncoder()\n",
    "        self.lr = RidgeCV()\n",
    "        self.classes_ = [-1,1]\n",
    "    def fit(self, X, y=None, **fit_params):\n",
    "        self.lgb.fit(X, y)\n",
    "        self.grd_enc.fit(self.lgb.apply(X))\n",
    "        self.lr.fit(self.grd_enc.transform(self.lgb.apply(X)), y)\n",
    "    def predict(self, X):\n",
    "        return self.lr.predict(self.grd_enc.transform(self.lgb.apply(X)))\n",
    "    \n",
    "tmp = myStackingFeatures()\n",
    "tmp.fit(train_x, train_y)\n",
    "r = tmp.predict(train_x)\n",
    "np.round(mean_squared_error(train_y, r), 5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[   1    2    3 ..., 5637 5638 5639]\n",
      "[   0    1    2 ..., 5638 5640 5641]\n",
      "6.06 5.47133469885\n"
     ]
    }
   ],
   "source": [
    "#print(r[33],r[313],r[928],r[938])\n",
    "#print(r.min())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 216,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\altman\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py:642: UserWarning: max_bin keyword has been found in `params` and will be ignored. Please use max_bin argument of the Dataset constructor to pass this parameter.\n",
      "  'Please use {0} argument of the Dataset constructor to pass this parameter.'.format(key))\n",
      "C:\\Users\\altman\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py:648: LGBMDeprecationWarning: The `max_bin` parameter is deprecated and will be removed in 2.0.12 version. Please use `params` to pass this parameter.\n",
      "  'Please use `params` to pass this parameter.', LGBMDeprecationWarning)\n",
      "C:\\Users\\altman\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py:642: UserWarning: max_bin keyword has been found in `params` and will be ignored. Please use max_bin argument of the Dataset constructor to pass this parameter.\n",
      "  'Please use {0} argument of the Dataset constructor to pass this parameter.'.format(key))\n",
      "C:\\Users\\altman\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py:648: LGBMDeprecationWarning: The `max_bin` parameter is deprecated and will be removed in 2.0.12 version. Please use `params` to pass this parameter.\n",
      "  'Please use `params` to pass this parameter.', LGBMDeprecationWarning)\n",
      "C:\\Users\\altman\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py:642: UserWarning: max_bin keyword has been found in `params` and will be ignored. Please use max_bin argument of the Dataset constructor to pass this parameter.\n",
      "  'Please use {0} argument of the Dataset constructor to pass this parameter.'.format(key))\n",
      "C:\\Users\\altman\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py:648: LGBMDeprecationWarning: The `max_bin` parameter is deprecated and will be removed in 2.0.12 version. Please use `params` to pass this parameter.\n",
      "  'Please use `params` to pass this parameter.', LGBMDeprecationWarning)\n",
      "C:\\Users\\altman\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py:642: UserWarning: max_bin keyword has been found in `params` and will be ignored. Please use max_bin argument of the Dataset constructor to pass this parameter.\n",
      "  'Please use {0} argument of the Dataset constructor to pass this parameter.'.format(key))\n",
      "C:\\Users\\altman\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py:648: LGBMDeprecationWarning: The `max_bin` parameter is deprecated and will be removed in 2.0.12 version. Please use `params` to pass this parameter.\n",
      "  'Please use `params` to pass this parameter.', LGBMDeprecationWarning)\n",
      "C:\\Users\\altman\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py:642: UserWarning: max_bin keyword has been found in `params` and will be ignored. Please use max_bin argument of the Dataset constructor to pass this parameter.\n",
      "  'Please use {0} argument of the Dataset constructor to pass this parameter.'.format(key))\n",
      "C:\\Users\\altman\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py:648: LGBMDeprecationWarning: The `max_bin` parameter is deprecated and will be removed in 2.0.12 version. Please use `params` to pass this parameter.\n",
      "  'Please use `params` to pass this parameter.', LGBMDeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 0.94242\n"
     ]
    }
   ],
   "source": [
    "N = 5\n",
    "kf = KFold(n_splits=N, random_state=42)\n",
    "result_mean = 0.0\n",
    "for train_index, test_index in kf.split(train_x):\n",
    "    training_features, training_target = train_x[train_index], train_y[train_index]\n",
    "    testing_features, testing_target  = train_x[test_index], train_y[test_index]\n",
    "    \n",
    "    exported_pipeline = Pipeline([\n",
    "        (\"scaler\", MaxAbsScaler()),\n",
    "        (\"SVR\", StackingEstimator(estimator=LinearSVR(C=0.01, dual=False, epsilon=1.0, loss=\"squared_epsilon_insensitive\", tol=0.001))),\n",
    "        (\"RidgeCV\", StackingEstimator(estimator=RidgeCV())),\n",
    "        (\"LGB\", lgb.LGBMRegressor(objective='regression',\n",
    "                                  boosting_type=\"GBDT\",\n",
    "                                  num_leaves=17,\n",
    "                                  learning_rate=0.01,\n",
    "                                  feature_fraction=0.5,\n",
    "                                  bagging_fraction=0.5,\n",
    "                                  bagging_freq=5,\n",
    "                                  reg_alpha=0.1,\n",
    "                                  reg_lambda=0.1,\n",
    "                                  n_estimators=400))]\n",
    "    )\n",
    "    exported_pipeline.fit(training_features, training_target)\n",
    "    results = exported_pipeline.predict(testing_features)   \n",
    "    \n",
    "    \n",
    "    mystacking = myStackingFeatures()\n",
    "    mystacking.fit(training_features, training_target)\n",
    "    new_results = mystacking.predict(testing_features)  \n",
    "    \n",
    "    results = 0.8*results + 0.2*new_results\n",
    "    \n",
    "\n",
    "    \n",
    "    #negative_results,negative_pred_list, positive_results,positive_pred_list = modif_value(training_features, training_target, high_labels[train_index], testing_features)\n",
    "    \n",
    "    #if len(negative_results) !=0 and len(negative_pred_list)!=0:\n",
    "    #    for i,j in enumerate(negative_pred_list):\n",
    "    #        results[j] = negative_results[i]\n",
    "        \n",
    "            \n",
    "                       \n",
    "    result_mean += np.round(mean_squared_error(testing_target, results), 5)\n",
    "result_mean /= (N)\n",
    "print(\"Mean squared error: %.5f\" % (result_mean/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(100,)"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\altman\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py:642: UserWarning: max_bin keyword has been found in `params` and will be ignored. Please use max_bin argument of the Dataset constructor to pass this parameter.\n",
      "  'Please use {0} argument of the Dataset constructor to pass this parameter.'.format(key))\n",
      "C:\\Users\\altman\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py:648: LGBMDeprecationWarning: The `max_bin` parameter is deprecated and will be removed in 2.0.12 version. Please use `params` to pass this parameter.\n",
      "  'Please use `params` to pass this parameter.', LGBMDeprecationWarning)\n",
      "C:\\Users\\altman\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py:642: UserWarning: max_bin keyword has been found in `params` and will be ignored. Please use max_bin argument of the Dataset constructor to pass this parameter.\n",
      "  'Please use {0} argument of the Dataset constructor to pass this parameter.'.format(key))\n",
      "C:\\Users\\altman\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py:648: LGBMDeprecationWarning: The `max_bin` parameter is deprecated and will be removed in 2.0.12 version. Please use `params` to pass this parameter.\n",
      "  'Please use `params` to pass this parameter.', LGBMDeprecationWarning)\n",
      "C:\\Users\\altman\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py:642: UserWarning: max_bin keyword has been found in `params` and will be ignored. Please use max_bin argument of the Dataset constructor to pass this parameter.\n",
      "  'Please use {0} argument of the Dataset constructor to pass this parameter.'.format(key))\n",
      "C:\\Users\\altman\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py:648: LGBMDeprecationWarning: The `max_bin` parameter is deprecated and will be removed in 2.0.12 version. Please use `params` to pass this parameter.\n",
      "  'Please use `params` to pass this parameter.', LGBMDeprecationWarning)\n",
      "C:\\Users\\altman\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py:642: UserWarning: max_bin keyword has been found in `params` and will be ignored. Please use max_bin argument of the Dataset constructor to pass this parameter.\n",
      "  'Please use {0} argument of the Dataset constructor to pass this parameter.'.format(key))\n",
      "C:\\Users\\altman\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py:648: LGBMDeprecationWarning: The `max_bin` parameter is deprecated and will be removed in 2.0.12 version. Please use `params` to pass this parameter.\n",
      "  'Please use `params` to pass this parameter.', LGBMDeprecationWarning)\n",
      "C:\\Users\\altman\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py:642: UserWarning: max_bin keyword has been found in `params` and will be ignored. Please use max_bin argument of the Dataset constructor to pass this parameter.\n",
      "  'Please use {0} argument of the Dataset constructor to pass this parameter.'.format(key))\n",
      "C:\\Users\\altman\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py:648: LGBMDeprecationWarning: The `max_bin` parameter is deprecated and will be removed in 2.0.12 version. Please use `params` to pass this parameter.\n",
      "  'Please use `params` to pass this parameter.', LGBMDeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 0.94521\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"../data/processed/train_best.csv\")\n",
    "test = pd.read_csv(\"../data/processed/test_best.csv\")\n",
    "test.pop(\"id\")\n",
    "train.pop(\"id\")\n",
    "target = train.pop(\"血糖\")\n",
    "train_x = train.as_matrix()\n",
    "train_y = target.as_matrix()\n",
    "test_x = test.as_matrix()\n",
    "N = 5\n",
    "kf = KFold(n_splits=N, random_state=42)\n",
    "result_mean = 0.0\n",
    "for train_index, test_index in kf.split(train_x):\n",
    "    training_features, training_target = train_x[train_index], train_y[train_index]\n",
    "    testing_features, testing_target  = train_x[test_index], train_y[test_index]\n",
    "    exported_pipeline = make_pipeline(\n",
    "    MaxAbsScaler(),\n",
    "    StackingEstimator(estimator=LinearSVR(C=0.01, dual=False, epsilon=1.0, loss=\"squared_epsilon_insensitive\", tol=0.001)),\n",
    "    StackingEstimator(estimator=RidgeCV()),\n",
    "    lgb.LGBMRegressor(objective='regression',\n",
    "                    boosting_type =\"GBDT\",\n",
    "                    num_leaves=17,\n",
    "                    learning_rate=0.01,\n",
    "                    feature_fraction=0.5,\n",
    "                    bagging_fraction=0.5,\n",
    "                    bagging_freq=5,\n",
    "                    reg_alpha=1,\n",
    "                    reg_lambda=0.5,\n",
    "                    n_estimators=500)\n",
    "    )\n",
    "    exported_pipeline.fit(training_features, training_target)\n",
    "    results = exported_pipeline.predict(testing_features)    \n",
    "    \n",
    "\n",
    "    \n",
    "    result_mean += np.round(mean_squared_error(testing_target, results), 5)\n",
    "result_mean /= (N)\n",
    "print(\"Mean squared error: %.5f\" % (result_mean/2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\altman\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py:642: UserWarning: max_bin keyword has been found in `params` and will be ignored. Please use max_bin argument of the Dataset constructor to pass this parameter.\n",
      "  'Please use {0} argument of the Dataset constructor to pass this parameter.'.format(key))\n",
      "C:\\Users\\altman\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py:648: LGBMDeprecationWarning: The `max_bin` parameter is deprecated and will be removed in 2.0.12 version. Please use `params` to pass this parameter.\n",
      "  'Please use `params` to pass this parameter.', LGBMDeprecationWarning)\n",
      "C:\\Users\\altman\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py:642: UserWarning: max_bin keyword has been found in `params` and will be ignored. Please use max_bin argument of the Dataset constructor to pass this parameter.\n",
      "  'Please use {0} argument of the Dataset constructor to pass this parameter.'.format(key))\n",
      "C:\\Users\\altman\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py:648: LGBMDeprecationWarning: The `max_bin` parameter is deprecated and will be removed in 2.0.12 version. Please use `params` to pass this parameter.\n",
      "  'Please use `params` to pass this parameter.', LGBMDeprecationWarning)\n",
      "C:\\Users\\altman\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py:642: UserWarning: max_bin keyword has been found in `params` and will be ignored. Please use max_bin argument of the Dataset constructor to pass this parameter.\n",
      "  'Please use {0} argument of the Dataset constructor to pass this parameter.'.format(key))\n",
      "C:\\Users\\altman\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py:648: LGBMDeprecationWarning: The `max_bin` parameter is deprecated and will be removed in 2.0.12 version. Please use `params` to pass this parameter.\n",
      "  'Please use `params` to pass this parameter.', LGBMDeprecationWarning)\n",
      "C:\\Users\\altman\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py:642: UserWarning: max_bin keyword has been found in `params` and will be ignored. Please use max_bin argument of the Dataset constructor to pass this parameter.\n",
      "  'Please use {0} argument of the Dataset constructor to pass this parameter.'.format(key))\n",
      "C:\\Users\\altman\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py:648: LGBMDeprecationWarning: The `max_bin` parameter is deprecated and will be removed in 2.0.12 version. Please use `params` to pass this parameter.\n",
      "  'Please use `params` to pass this parameter.', LGBMDeprecationWarning)\n",
      "C:\\Users\\altman\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py:642: UserWarning: max_bin keyword has been found in `params` and will be ignored. Please use max_bin argument of the Dataset constructor to pass this parameter.\n",
      "  'Please use {0} argument of the Dataset constructor to pass this parameter.'.format(key))\n",
      "C:\\Users\\altman\\Anaconda3\\lib\\site-packages\\lightgbm\\basic.py:648: LGBMDeprecationWarning: The `max_bin` parameter is deprecated and will be removed in 2.0.12 version. Please use `params` to pass this parameter.\n",
      "  'Please use `params` to pass this parameter.', LGBMDeprecationWarning)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean squared error: 0.93706\n"
     ]
    }
   ],
   "source": [
    "train = pd.read_csv(\"../data/processed/train.csv\")\n",
    "test = pd.read_csv(\"../data/processed/test.csv\")\n",
    "test.pop(\"id\")\n",
    "train.pop(\"id\")\n",
    "target = train.pop(\"血糖\")\n",
    "train_x = train.as_matrix()\n",
    "train_y = target.as_matrix()\n",
    "test_x = test.as_matrix()\n",
    "N = 5\n",
    "kf = KFold(n_splits=N, random_state=42)\n",
    "result_mean = 0.0\n",
    "for train_index, test_index in kf.split(train_x):\n",
    "    training_features, training_target = train_x[train_index], train_y[train_index]\n",
    "    testing_features, testing_target  = train_x[test_index], train_y[test_index]\n",
    "    exported_pipeline = Pipeline([\n",
    "        (\"scaler\", MaxAbsScaler()),\n",
    "        (\"SVR\", StackingEstimator(estimator=LinearSVR(C=0.01, dual=False, epsilon=1.0, loss=\"squared_epsilon_insensitive\", tol=0.001))),\n",
    "        (\"RidgeCV\", StackingEstimator(estimator=RidgeCV())),\n",
    "        (\"LGB\", lgb.LGBMRegressor(objective='regression',\n",
    "                                  boosting_type=\"GBDT\",\n",
    "                                  num_leaves=31,\n",
    "                                  learning_rate=0.01,\n",
    "                                  feature_fraction=0.5,\n",
    "                                  bagging_fraction=0.5,\n",
    "                                  bagging_freq=5,\n",
    "                                  n_estimators=400))]\n",
    "    )\n",
    "    exported_pipeline.fit(training_features, training_target)\n",
    "    results = exported_pipeline.predict(testing_features)    \n",
    "    \n",
    "\n",
    "    \n",
    "    result_mean += np.round(mean_squared_error(testing_target, results), 5)\n",
    "result_mean /= (N)\n",
    "print(\"Mean squared error: %.5f\" % (result_mean/2))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
